# Giving Design Advice

 
In an ideal world, designers freely seek advice, ask for and offer constructive criticism, and openly discuss issues. They don't take criticism as personal affronts, and they and their managers make intelligent, informed decisions. But in reality, it can be difficult to get others to acknowledge criticism or heed advice. How can you convey the importance of an issue, convince others to take action, or get them to recognize a proposed alternative as the better choice?

 

 
## Cognitive biases

 
**Cognitive biases** affect how people naturally receive and process information. They can prevent people from appropriately acting on information or making rational decisions by causing a person to ignore evidence, disavow or exaggerate risk, or be unduly swayed by inconsequential details. The difficulty is in identifying a person's biases, since not everyone shares the same ones. I've been confounded at times after failing to effectively communicate constructive criticism. In hindsight, I suspect that had I considered certain cognitive biases, I could have argued more persuasively.

 
Cognitive biases aren't caused by an emotional or intellectual predisposition toward making a particular decision. Nevertheless, when people process information, their interpretation might be colored by their cognitive biases. Here I discuss cognitive biases common to design discussions. (For a more complete list of such biases, see <span style="color:blue"><a href="http://en.wikipedia.org/wiki/cognitive_biases" target="_blank">http://en.wikipedia.org/wiki/cognitive_biases</a></span>.)

 <h3>Confirmation bias</h3> 
A **confirmation bias** is when someone tends to notice or look for what confirms a strongly held belief while ignoring or undervaluing contradictory evidence. Have you ever found yourself unable to isolate a software bug because you insisted that things just couldn't work that way? Your confirmation bias might have prevented you from noticing facts that would lead you to the bug's root cause.

 
When beliefs are based on evidence gleaned from extensive experimentation, a tendency to give undue attention and weight to data that fits those beliefs is a pretty good strategy. Furthermore, we increase our efficiency in processing data when we can quickly ignore irrelevant information. However, we can become close-minded when we refuse to let new facts or ways of viewing a situation enter into our decision making.

 
So don't be surprised when your colleagues refuse to accept criticism if it includes information that radically differs from their tightly held beliefs. Such beliefs will be difficult (but not impossible) to change. Convincing the person of your argument's merits will require much more than a single, impassioned appeal to reason. Furthermore, the worst thing you could do is make an all-or-nothing statement, which just puts the person on the defense.

 
Instead of attacking a design's flaws, one way to subtly point out its weaknesses is to ask clarifying questions: "Hmm, why did you do this the way you did? Maybe I'm just confused, but I don't see how your decision would handle this situation. Did you think about that?" This lets your colleagues explain their rationale without feeling threatened, and as they explain their reasoning, you can ask more probing questions. Peter Falk, who played a rumpled homicide detective on the TV show **Columbo**, was a master at saying, "Oh, just one more thing …." I'm not suggesting you pepper your colleagues with a barrage of annoying clarifying questions; rather, occasionally employ this tactic to initiate a meaningful conversation. Your goal should be to get them to realize either one or more weaknesses in their design or inconsistencies in their defense of a particular design choice.

 <h3>The contrast effect</h3> 
There are times, however, when you need to be blunt. Consider when you want to make it perfectly clear that one option is superior. How can you do that? Contrast it with an inferior (but not ridiculous) option. Given two options, people tend to evaluate them against one another rather than against known standards. It also helps to present your preferred option second. People will generally be more receptive to a second option if it's presented immediately after another one. (However, choosing the second alternative isn't always a good thing. To avoid falling into this trap yourself, pause and critically think about the comparison standards you're using when judging the two options.)

 <h3>Information bias</h3> 
Sometimes, no matter how compelling your arguments, people are reluctant to make decisions. Instead, they just ask for more evidence. Early in my career, I worked for a manager who believed that the more information he could acquire before making a decision, the better. Every time I'd present a recommendation and ask for a decision, he'd ask for more data. Did he not trust me, was he stalling, or did he suffer from **information bias?**

 
Often extra information won't affect the outcome, but someone who shows a strong information bias will ask for more information, especially in times of uncertainty. As designers, we often make decisions based on partial or incomplete information. We must do what we can on the basis of what we know at the time.

 
How can you counteract a severe case of information bias? Add a few new facts to your recommendation or observations. Or present your recommendation as fresh, even when you don't have anything substantive to add to it, by emphasizing certain parts of your recommendation or summarizing its benefits.

 
If that doesn't work, find a way to prove your advice. Recently, I spoke with an architect whose colleagues initially ignored his dire warnings that a proposed feature wasn't feasible. Only by developing a prototype and demonstrating its behavior could he convince his colleagues that the feature would severely degrade system performance. This illustrates another bias at work—the **neglect of probability** bias. Those who hold this bias discount any risk that they perceive as being less than certain. When the architect offered indisputable evidence, his advice was heeded.

 <h3>Hyperbolic discounting</h3> 
Sometimes you can't easily prove a recommendation. So you have to couch your advice in terms of estimated effortand future benefits. People prefer options that offer smaller rewards with more immediate payoff to options with larger rewards promised in the future. (However, if the payoff is far enough in the future, people prefer the option with the larger reward if the time difference between the two options isn't too great.)

 
Psychologists have found that most people allocate their time and effort between two nonexclusive, ongoing sources of reward in direct proportion to the rate and size of those rewards, and in inverse proportion to the delay in receiving them. This preference follows a hyperbolic curve—hence the name **hyperbolic discounting**. Be warned: it will be extremely difficult for you to get others to choose an alternative that involves extra effort over a lesser effort with a more immediate reward. That's why management—and even most designers—will almost always select quick fixes and simple design refactorings over extensive redesign recommendations (and why health fads promising immediate benefits are eternally popular).

 <h3>Primacy and recency effects</h3> 
Sometimes you have a number of recommendations. How can you present them so that the most important items are obvious, increasing the likelihood that they will be acted upon? Given a list of items, people tend to remember the first and last few items more than those in the middle. People start out listening or reading attentively, but they can get bored or distracted as they process information.

 
To make an item really stand out, state it first (it will be remembered because of the **primacy effect**) and restate it again at the end (to leverage the **recency effect**). Don't let an important fact or recommendation get buried in the middle of a long list.

 

 
## Decision-making under duress

 
Many situations exist in which people unconsciously make irrational judgments. If you're aware of how people naturally process information, you can tweak your presentation accordingly. However, mood can also greatly influence judgment. Contented designers are more likely to judge options and recommendations in a positive light. When tired or stressed, people are less likely to have a positive attitude.

 
I've witnessed this during several lengthy conference committee meetings. Inevitably, there's a point in the meeting when people start to tire and judge submissions more harshly. In fact, once one person starts being harsh, others join in (the **bandwagon effect**) and things can take a nasty turn. Experienced committee chairs will tell you that just before this happens is when to stop the meeting. Reconvene in the morning when people are well-rested, fed, and in a more positive mood. When conducting design reviews or workshops, I always suggest a break when I notice people getting distracted, tired, or cranky. Grumpy people rarely weigh options in a positive light.

 

 
## Conclusion

 
By becoming aware of some common cognitive biases, you can learn when it's worthwhile to tweak your message to increase the likelihood of its acceptance. Even if people don't always follow your advice, it helps to understand that their negative reactions might have little to do with you (and everything to do with how they naturally process information). Cognitive biases exist, and we designers are remiss if we ignore them. Reframing advice so that people are more likely to follow it isn't sneaky or manipulative—it's common sense. We should employ every device possible so that our information, argumentation, and advice are clearly understood.

 

